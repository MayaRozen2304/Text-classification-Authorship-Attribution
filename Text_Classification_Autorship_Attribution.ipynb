{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kEVPMcrM0uui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d6bce4-4d1d-49cc-9e29-d8b0383862d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "# PyTprch and Transformers installation and import\n",
        "# These are preinstalled so you will get \"Requirement already satisfied\" but nevertheless, it is required before you import the relevant packages.\n",
        "# (not needed for the basic classificaiton models)\n",
        "\n",
        "! pip install transformers datasets\n",
        "! pip3 install torch\n",
        "!pip install scipy\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import csv\n",
        "import urllib.request\n",
        "import nltk, scipy\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from xgboost import XGBClassifier\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n"
      ],
      "metadata": {
        "id": "tkS9XJHo1q9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69467288-0f1a-4757-ff04-f1f72454c4bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithmic approaches:**\n",
        "\n",
        "\n",
        "\n",
        "1. [sklearn.linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)\n",
        "2. [sklearn.svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) (try both linear and nonlinear kernels!)\n",
        "3. FFNN classifier - You should use  the PyTorch library to build a FFNN classifier (with at least one hidden layer) to achieve the classification. Feel free to experiment with the number of layers ([a simple tutorial for FFNN with PyTorch](https://medium.com/biaslyai/pytorch-introduction-to-neural-network-feedforward-neural-network-model-e7231cff47cb)).\n",
        "4. A fourth classifier of choice (neural or not). You are encouraged to experiment with classifiers that allow combining different types of features (e.g. number of capitalized words, time of tweeting, etc.)\n",
        "5. A fifth classifier of your choice  (this should be neural -  RNN, or transformer-based) - feel free to experiment.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qy428MPY1r__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(file_path, tweet_split_delimiter='\\n'):\n",
        "    \"\"\"\n",
        "    Reads a TSV training file, splits rows with multiple tweets into separate rows,\n",
        "    and applies binary labeling based on user_handle and device.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: str - path to the input file\n",
        "    - tweet_split_delimiter: str - delimiter used to split multiple tweets in one row (default: ' || ')\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with separate rows for each tweet and a binary 'label' column.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    df = pd.read_csv(file_path, sep='\\t', header=None)\n",
        "    df.columns = ['tweet_id', 'user_handle', 'tweet_text', 'timestamp', 'device']\n",
        "\n",
        "    # Ensure all tweet_texts are string and split multi-tweet entries\n",
        "    df['tweet_text'] = df['tweet_text'].astype(str).str.split(tweet_split_delimiter)\n",
        "\n",
        "    # Explode to separate rows\n",
        "    df = df.explode('tweet_text').reset_index(drop=True)\n",
        "\n",
        "    # Trim extra whitespace\n",
        "    df['tweet_text'] = df['tweet_text'].str.strip()\n",
        "\n",
        "    # Create binary label\n",
        "    df['label'] = df.apply(\n",
        "        lambda row: 0 if row['user_handle'] == 'realDonaldTrump' and row['device'].lower() == 'android' else 1,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "Wg7I4eYyWhts"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_test_data(file_path, tweet_split_delimiter='\\n'):\n",
        "    \"\"\"\n",
        "    Reads a TSV test file and splits multiple tweets per row (if present) into separate rows.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: str - path to the test file\n",
        "    - tweet_split_delimiter: str - separator used for multiple tweets per row (default: '\\n')\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with one tweet per row, and appropriate columns assigned\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(file_path, sep='\\t', header=None)\n",
        "    col_count = df.shape[1]\n",
        "    if col_count == 3: #test\n",
        "      df.columns = ['user_handle', 'tweet_text', 'timestamp']\n",
        "    elif col_count ==5: #cross validation\n",
        "      df.columns = ['tweet_id', 'user_handle', 'tweet_text', 'timestamp', 'device']\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected number of columns: {col_count}\")\n",
        "\n",
        "    # Ensure all tweet_texts are string and split multi-tweet entries\n",
        "    df['tweet_text'] = df['tweet_text'].astype(str).str.split(tweet_split_delimiter)\n",
        "\n",
        "    # Explode to separate rows\n",
        "    df = df.explode('tweet_text').reset_index(drop=True)\n",
        "\n",
        "    # Trim extra whitespace\n",
        "    df['tweet_text'] = df['tweet_text'].str.strip()\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "0-ug1Tajf4Aq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df, time_stamp_col='timestamp', hour_sep=3):\n",
        "    \"\"\"\n",
        "    Extracts and adds raw text-based, cleaned text-based, and time-based features from a DataFrame of tweets.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pd.DataFrame - input DataFrame with 'tweet_text' and a timestamp column\n",
        "    - time_stamp_col: str - name of the timestamp column\n",
        "    - hour_sep: int - interval (in hours) to categorize timestamps into time bins\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame enriched with engineered text and time features, and missing values handled.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # ---------- Step 1: RAW TEXT-BASED FEATURES (before cleaning) ----------\n",
        "    def count_raw_features(text):\n",
        "        return pd.Series({\n",
        "            'uppercase_chars': sum(1 for c in text if c.isupper()),\n",
        "            'uppercase_words': sum(1 for w in text.split() if w.isupper()),\n",
        "            'quotation_marks': text.count('\"'),\n",
        "            'at_signs': text.count('@'),\n",
        "            'hashtags': text.count('#'),\n",
        "            'digit_chars': sum(1 for c in text if c.isdigit()),\n",
        "            'question_marks': text.count('?'),\n",
        "            'exclamation_marks': text.count('!'),\n",
        "            'retweet_count': len(re.findall(r'\\bRT\\b', text))\n",
        "        })\n",
        "\n",
        "    raw_feats = df['tweet_text'].astype(str).apply(count_raw_features)\n",
        "\n",
        "    # ---------- Step 2: CLEAN TEXT ----------\n",
        "    def clean_text(text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "        return text\n",
        "\n",
        "    df['clean_text'] = df['tweet_text'].astype(str).apply(clean_text)\n",
        "\n",
        "    # ---------- Step 3: CLEAN TEXT-BASED FEATURES ----------\n",
        "    def count_clean_features(text):\n",
        "        words = text.split()\n",
        "        num_words = len(words)\n",
        "        num_chars = len(text)\n",
        "        num_stopwords = sum(1 for word in words if word in stop_words)\n",
        "        avg_word_len = (num_chars / num_words) if num_words > 0 else 0\n",
        "\n",
        "        return pd.Series({\n",
        "            'char_length': num_chars,\n",
        "            'word_length': num_words,\n",
        "            'char_word_ratio': avg_word_len,\n",
        "            'stopword_count': num_stopwords\n",
        "        })\n",
        "\n",
        "    clean_feats = df['clean_text'].apply(count_clean_features)\n",
        "\n",
        "    # ---------- Step 4: TIME-BASED FEATURES ----------\n",
        "    df[time_stamp_col] = pd.to_datetime(df[time_stamp_col], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "\n",
        "    time_dist = list(range(0, 25, hour_sep))\n",
        "    df.loc[:, 'hour_cat'] = pd.cut(df[time_stamp_col].dt.hour, bins=time_dist, labels=False, right=False)\n",
        "    df.loc[:, 'day_of_week'] = df[time_stamp_col].dt.dayofweek\n",
        "    df.loc[:, 'weekend'] = np.where(df['day_of_week'].isin([5, 6]), 1, 0)\n",
        "\n",
        "    # ---------- Final: merge everything ----------\n",
        "    df = pd.concat([df, raw_feats, clean_feats], axis=1)\n",
        "\n",
        "        # ---------- Step 5: Fill any remaining missing values ----------\n",
        "    df = df.fillna(df.mean(numeric_only=True))  # Fill numeric NaNs with mean\n",
        "    df = df.fillna(\"unknown\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "zb3s_WGlaDYk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_features(df, feature_cols, scaler=None, fit=True):\n",
        "    \"\"\"\n",
        "    Normalizes selected columns in the DataFrame using StandardScaler.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame containing the features.\n",
        "        feature_cols (list): List of column names to normalize.\n",
        "        scaler (StandardScaler, optional): Pre-fitted scaler for transformation. If None, will create a new one.\n",
        "        fit (bool): Whether to fit the scaler on this DataFrame (True for training, False for test/val).\n",
        "\n",
        "    Returns:\n",
        "        X_scaled (np.ndarray): Normalized feature matrix.\n",
        "        scaler (StandardScaler): The fitted scaler (to use on other datasets).\n",
        "    \"\"\"\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "    X = df[feature_cols].copy()\n",
        "\n",
        "    if fit:\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "    else:\n",
        "        X_scaled = scaler.transform(X)\n",
        "\n",
        "    return X_scaled, scaler\n"
      ],
      "metadata": {
        "id": "q7Csf6-Ec-70"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = [\n",
        "    'uppercase_chars', 'uppercase_words', 'quotation_marks', 'at_signs', 'hashtags',\n",
        "    'digit_chars', 'question_marks', 'exclamation_marks', 'retweet_count',\n",
        "    'char_length', 'word_length', 'char_word_ratio', 'stopword_count',\n",
        "    'hour_cat', 'day_of_week', 'weekend'\n",
        "]\n"
      ],
      "metadata": {
        "id": "nY9uEsArdGMv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Training Function - (Logistic Regression, SVM,Xgboost)**\n"
      ],
      "metadata": {
        "id": "7gEEbm7eqNKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_model(train_path,model_type):\n",
        "    \"\"\"\n",
        "    Trains a classification model using engineered text, metadata, and time-based features.\n",
        "\n",
        "    Parameters:\n",
        "    - train_path: str - path to the training file\n",
        "    - model_type: sklearn-like estimator\n",
        "\n",
        "    Returns:\n",
        "    - model: trained model\n",
        "    - vectorizer: fitted TfidfVectorizer used for text transformation\n",
        "    - scaler: fitted scaler used for normalizing numeric features\n",
        "    \"\"\"\n",
        "    df_train = read_data(train_path)\n",
        "    df_train = preprocess_data(df_train)\n",
        "    df_train['tweet_text'] = df_train['tweet_text'].fillna(\"\")\n",
        "    feature_cols = [\n",
        "        'uppercase_chars', 'uppercase_words', 'quotation_marks', 'at_signs', 'hashtags',\n",
        "        'digit_chars', 'question_marks', 'exclamation_marks', 'retweet_count',\n",
        "        'char_length', 'word_length', 'char_word_ratio', 'stopword_count',\n",
        "        'hour_cat', 'day_of_week', 'weekend'\n",
        "    ]\n",
        "\n",
        "    # Fill missing numeric features with their column mean\n",
        "    for col in feature_cols:\n",
        "        mean_value = df_train[col].mean()\n",
        "        df_train[col] = df_train[col].fillna(mean_value)\n",
        "\n",
        "    # Normalize\n",
        "    X_train_scaled, scaler = normalize_features(df_train, feature_cols, fit=True)\n",
        "\n",
        "    # Text vectorization\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_tfidf = vectorizer.fit_transform(df_train['tweet_text'])\n",
        "\n",
        "    # Combine features\n",
        "    X_combined = hstack([X_tfidf, csr_matrix(X_train_scaled)])\n",
        "    y_train = df_train['label']\n",
        "\n",
        "    # Train model\n",
        "    model = model_type\n",
        "    model.fit(X_combined, y_train)\n",
        "\n",
        "    return model, vectorizer, scaler\n"
      ],
      "metadata": {
        "id": "Zlb_nAigqUvc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FFNN**"
      ],
      "metadata": {
        "id": "XhD32-0_mAj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FFNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A feedforward neural network for binary classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=128):\n",
        "        super(FFNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "RMNNEzQErtXk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFNNWrapper:\n",
        "    \"\"\"\n",
        "    Wrapper class for a trained FFNN model.\n",
        "\n",
        "    Parameters:\n",
        "    - model: trained FFNN instance\n",
        "    - vectorizer: fitted TfidfVectorizer\n",
        "    - scaler: fitted StandardScaler\n",
        "    - feature_cols: list of structured feature column names\n",
        "\n",
        "    Methods:\n",
        "    - predict_from_raw_texts(df): runs predictions on a raw DataFrame\n",
        "    \"\"\"\n",
        "    def __init__(self, model, vectorizer, scaler, feature_cols):\n",
        "        self.model = model\n",
        "        self.my_vectorizer = vectorizer\n",
        "        self.my_scaler = scaler\n",
        "        self.my_feature_cols = feature_cols\n",
        "\n",
        "    def predict_from_raw_texts(self, df):\n",
        "        df = preprocess_data(df)\n",
        "\n",
        "        X_tfidf = self.my_vectorizer.transform(df['tweet_text'])\n",
        "        X_extra = df[self.my_feature_cols].fillna(0)\n",
        "        X_scaled = self.my_scaler.transform(X_extra)\n",
        "        X_combined = hstack([X_tfidf, csr_matrix(X_scaled)]).toarray()\n",
        "\n",
        "        self.model.eval()\n",
        "        X_tensor = torch.tensor(X_combined).float()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X_tensor).squeeze()\n",
        "            preds = (outputs >= 0.5).int().tolist()\n",
        "        return preds\n",
        "\n",
        "def train_ffnn(train_path, hidden_dim=128, epochs=10, batch_size=32, lr=0.001):\n",
        "    \"\"\"\n",
        "    Trains a feedforward neural network on TF-IDF and structured features.\n",
        "\n",
        "    Parameters:\n",
        "    - train_path: str - path to the training file\n",
        "    - hidden_dim: int - number of units in the hidden layer\n",
        "    - epochs: int - number of training epochs\n",
        "    - batch_size: int - size of training batches\n",
        "    - lr: float - learning rate for optimizer\n",
        "\n",
        "    Returns:\n",
        "    - FFNNWrapper object containing the trained model and preprocessing tools\n",
        "    \"\"\"\n",
        "    df = read_data(train_path)\n",
        "    df = preprocess_data(df)\n",
        "\n",
        "    feature_cols = [\n",
        "        'uppercase_chars', 'uppercase_words', 'quotation_marks', 'at_signs', 'hashtags',\n",
        "        'digit_chars', 'question_marks', 'exclamation_marks', 'retweet_count',\n",
        "        'char_length', 'word_length', 'char_word_ratio', 'stopword_count',\n",
        "        'hour_cat', 'day_of_week', 'weekend'\n",
        "    ]\n",
        "\n",
        "    # Normalize structured features\n",
        "    scaler = StandardScaler()\n",
        "    X_extra = df[feature_cols].fillna(0)\n",
        "    X_scaled = scaler.fit_transform(X_extra)\n",
        "\n",
        "    # TF-IDF vectorization\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_tfidf = vectorizer.fit_transform(df['tweet_text'])\n",
        "\n",
        "    # Combine TF-IDF and structured features\n",
        "    X_combined = hstack([X_tfidf, csr_matrix(X_scaled)]).toarray()\n",
        "\n",
        "    y = df['label'].values\n",
        "    X_tensor = torch.tensor(X_combined).float()\n",
        "    y_tensor = torch.tensor(y).float().unsqueeze(1)\n",
        "\n",
        "    dataset = TensorDataset(X_tensor, y_tensor)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = FFNN(input_dim=X_combined.shape[1], hidden_dim=hidden_dim)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for xb, yb in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return FFNNWrapper(model, vectorizer, scaler, feature_cols)\n",
        "\n",
        "def predict_ffnn(model, test_path):\n",
        "    \"\"\"\n",
        "    Runs predictions using a trained FFNNWrapper on a test dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - model: FFNNWrapper - trained model with vectorizer and scaler\n",
        "    - test_path: str - path to the test file\n",
        "\n",
        "    Returns:\n",
        "    - List of binary predictions (0 or 1)\n",
        "    \"\"\"\n",
        "    df_test = read_test_data(test_path)\n",
        "    df_test = preprocess_data(df_test)\n",
        "\n",
        "    X_tfidf = model.my_vectorizer.transform(df_test['tweet_text']).toarray()\n",
        "    X_features = df_test[model.my_feature_cols].fillna(0).values  # just in case\n",
        "    X_scaled = model.my_scaler.transform(X_features)\n",
        "\n",
        "    X_combined = np.hstack([X_tfidf, X_scaled])\n",
        "    X_tensor = torch.tensor(X_combined).float()\n",
        "\n",
        "    model.model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.model(X_tensor).squeeze()\n",
        "        preds = (outputs >= 0.5).int().tolist()\n",
        "    return preds\n",
        "\n"
      ],
      "metadata": {
        "id": "MzAcZsnPn0bH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Roberta**"
      ],
      "metadata": {
        "id": "L0Gp5fu78UdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RobertaTextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for tokenizing and formatting text data for RoBERTa.\n",
        "\n",
        "    Parameters:\n",
        "    - texts: list or array-like - input text samples\n",
        "    - labels: list or array-like - corresponding labels\n",
        "    - tokenizer: HuggingFace tokenizer - tokenizer for RoBERTa\n",
        "    - max_length: int - maximum sequence length (default: 512)\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary per item with input_ids, attention_mask, and label tensor\n",
        "    \"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.labels = labels\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Convert texts to a list if it's not already\n",
        "        texts = texts.tolist() if hasattr(texts, 'tolist') else texts\n",
        "\n",
        "        # Pre-tokenize all texts\n",
        "        self.encodings = tokenizer(\n",
        "            texts,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            'input_ids': self.encodings['input_ids'][idx],\n",
        "            'attention_mask': self.encodings['attention_mask'][idx],\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "        return item\n",
        "\n",
        "def create_roberta_data_loader(data, tokenizer, batch_size=16, shuffle=True):\n",
        "    \"\"\"\n",
        "    Creates a DataLoader from tokenized text data for RoBERTa.\n",
        "\n",
        "    Parameters:\n",
        "    - data: pd.DataFrame - contain 'tweet_text' and 'label' columns\n",
        "    - tokenizer: HuggingFace tokenizer - tokenizer for RoBERTa\n",
        "    - batch_size: int - size of batches (default: 16)\n",
        "    - shuffle: bool - whether to shuffle the data (default: True)\n",
        "\n",
        "    Returns:\n",
        "    - PyTorch DataLoader for the RobertaTextDataset\n",
        "    \"\"\"\n",
        "    dataset = RobertaTextDataset(\n",
        "        texts=data['tweet_text'].values,\n",
        "        labels=data['label'].values,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    return data_loader\n",
        "\n",
        "def roberta_train(model, train_loader, optimizer, device, epochs=5):\n",
        "    \"\"\"\n",
        "    Fine-tunes a RoBERTa model using cross-entropy loss.\n",
        "\n",
        "    Parameters:\n",
        "    - model: RoBERTa model for sequence classification\n",
        "    - train_loader: DataLoader - training data loader\n",
        "    - optimizer: optimizer for model parameters\n",
        "    - device: torch.device - device to run training on\n",
        "    - epochs: int - number of training epochs (default: 5)\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        print(f'epoch:{epoch}')\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = torch.nn.functional.cross_entropy(outputs.logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "def train_roberta(data_path):\n",
        "    \"\"\"\n",
        "    Loads data, initializes tokenizer and RoBERTa model, and trains it.\n",
        "\n",
        "    Parameters:\n",
        "    - data_path: str - path to the training file\n",
        "\n",
        "    Returns:\n",
        "    - Trained RoBERTa model\n",
        "    \"\"\"\n",
        "    data = read_data(data_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained('FacebookAI/roberta-base')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('FacebookAI/roberta-base').to(device)\n",
        "    train_loader = create_roberta_data_loader(data, tokenizer, batch_size=16, shuffle=True)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    roberta_train(model, train_loader, optimizer, device, epochs=3)\n",
        "    return model\n",
        "\n",
        "\n",
        "def roberta_predict(model, test_path):\n",
        "    \"\"\"\n",
        "    Generates predictions using a fine-tuned RoBERTa model.\n",
        "\n",
        "    Parameters:\n",
        "    - model: trained RoBERTa model\n",
        "    - test_path: str - path to the test file\n",
        "\n",
        "    Returns:\n",
        "    - List of integer predictions (0 or 1)\n",
        "    \"\"\"\n",
        "    # Load tokenizer and device\n",
        "    tokenizer = AutoTokenizer.from_pretrained('FacebookAI/roberta-base')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    df_test = read_test_data(test_path)\n",
        "    # texts = df_test['tweet_text'].tolist()\n",
        "\n",
        "\n",
        "    predictions = []\n",
        "    # for text in texts:\n",
        "    for text in df_test['tweet_text']:\n",
        "        tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(tokens['input_ids'], tokens['attention_mask'])\n",
        "            _, predicted = torch.max(outputs.logits, dim=1)\n",
        "            predictions.append(predicted.item())\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "F_v9rdGhCES8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your notebook should support the following functions:"
      ],
      "metadata": {
        "id": "JMEN3rBq1_wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_pipeline(alg, train_fn):\n",
        "  \"\"\"Returns a trained model given the specific task and algorithm.\n",
        "      The pipeline should include all necessary steps that are needed for the\n",
        "      specified algoritm (preprocessing, normalization, feature extraction - depending\n",
        "      on your choice and decisions). Obviously, it is advised to implement the pipeline\n",
        "      through a sequence of function calls.\n",
        "\n",
        "    Args:\n",
        "\n",
        "        alg (int): an integer between 1-5, indicating the algorithmic approach as\n",
        "                    specified above (1: logistic regression, 2: svm, 3:FFNN, etc.).\n",
        "        train_ fn (str): full path to the file containing the training data.\n",
        "\n",
        "  \"\"\"\n",
        "  if alg ==1:\n",
        "    model_type = LogisticRegression(max_iter=1000)\n",
        "    m,v,s = train_model(train_fn,model_type)\n",
        "    m.my_vectorizer = v  # attach vectorizer to model object\n",
        "    m.my_scaler = s      # Attach scaler\n",
        "\n",
        "  elif alg == 2:\n",
        "    model_type= SVC()\n",
        "    m,v,s = train_model(train_fn,model_type)\n",
        "    m.my_vectorizer = v\n",
        "    m.my_scaler = s\n",
        "\n",
        "\n",
        "  elif alg == 3:\n",
        "    m = train_ffnn(train_fn)  # returns FFNNWrapper\n",
        "\n",
        "  elif alg == 4:\n",
        "    model_type= XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "    m, v,s = train_model(train_fn,model_type)\n",
        "    m.my_vectorizer = v\n",
        "    m.my_scaler = s\n",
        "\n",
        "  elif alg == 5:\n",
        "     m = train_roberta(train_fn)\n",
        "  return m"
      ],
      "metadata": {
        "id": "Dl2qo-UY2BNX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrain_best_model(train_fn = None):\n",
        "  \"\"\" Retrains and returns the best performing model for the specified task. The\n",
        "      function uses the hard coded settings you have found to work best for each\n",
        "      of the tasks.\n",
        "\n",
        "      Args:\n",
        "\n",
        "  \"\"\"\n",
        "  m = training_pipeline(5, train_fn)\n",
        "  return m"
      ],
      "metadata": {
        "id": "fXYZRHxK2MMu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(m, fn):\n",
        "    \"\"\"\n",
        "    Returns a list of 0s and 1s, corresponding to the lines in the specified file.\n",
        "\n",
        "    Args:\n",
        "      m: the trained model (may include .my_vectorizer and .my_scaler)\n",
        "      fn: the full path to a file in the same format as the test set\n",
        "    \"\"\"\n",
        "    if m.__class__.__name__ == 'FFNNWrapper':\n",
        "        y_pred = predict_ffnn(m, fn)\n",
        "\n",
        "    elif m.__class__.__name__ == 'RobertaForSequenceClassification':\n",
        "      y_pred = roberta_predict(m, fn)\n",
        "\n",
        "    else:\n",
        "        vectorizer = m.my_vectorizer\n",
        "        scaler = m.my_scaler\n",
        "        df_test = read_test_data(fn)\n",
        "        df_test = preprocess_data(df_test)\n",
        "        # Transform text\n",
        "        X_text = vectorizer.transform(df_test['tweet_text'])\n",
        "\n",
        "        # Normalize numeric features\n",
        "        X_numeric, _ = normalize_features(df_test, feature_cols, scaler=scaler, fit=False)\n",
        "\n",
        "        # Combine sparse (TF-IDF) and dense (numeric)\n",
        "        X_combined = hstack([X_text, X_numeric])\n",
        "\n",
        "        y_pred = m.predict(X_combined)\n",
        "\n",
        "    return y_pred\n"
      ],
      "metadata": {
        "id": "L6L0ZFSQ2Wjf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation -  5 Folds CV + Main()**"
      ],
      "metadata": {
        "id": "DMERGan3sEOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validate_model(alg, train_path, k=5):\n",
        "    \"\"\"\n",
        "    Performs k-fold cross-validation using the specified algorithm on raw TSV data.\n",
        "    Labels are created only for splitting and evaluation. Training and validation files remain unlabeled.\n",
        "\n",
        "    Args:\n",
        "        alg (int): ID of the algorithm to use (e.g., 1=LR, 2=SVM, 3=FFNN).\n",
        "        train_path (str): Path to the raw training TSV file.\n",
        "        k (int): Number of folds (default is 5).\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (mean_accuracy, mean_f1_score, mean_precision, mean_recall)\n",
        "    \"\"\"\n",
        "\n",
        "    df_raw = pd.read_csv(train_path, sep='\\t', header=None)\n",
        "    df_raw.columns = ['tweet_id', 'user_handle', 'tweet_text', 'timestamp', 'device']\n",
        "\n",
        "    # Ensure all tweet_texts are string and split multi-tweet entries\n",
        "    df_raw['tweet_text'] = df_raw['tweet_text'].astype(str).str.split('\\n')\n",
        "\n",
        "    # Explode to separate rows\n",
        "    df_raw = df_raw.explode('tweet_text').reset_index(drop=True)\n",
        "\n",
        "    # Trim extra whitespace\n",
        "    df_raw['tweet_text'] = df_raw['tweet_text'].str.strip()\n",
        "\n",
        "    # Create binary label\n",
        "    labels = df_raw.apply(\n",
        "        lambda row: 0 if row['user_handle'] == 'realDonaldTrump' and row['device'].lower() == 'android' else 1,\n",
        "        axis=1\n",
        "    ).tolist()\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    acc_scores = []\n",
        "    f1_scores = []\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(df_raw, labels)):\n",
        "        df_train = df_raw.iloc[train_idx].copy()  # training set without labels\n",
        "        df_val = df_raw.iloc[val_idx].copy()      # validation set without labels\n",
        "\n",
        "        # Save raw TSVs without label columns\n",
        "        df_train.to_csv(\"temp_train.tsv\", sep='\\t', header=False, index=False)\n",
        "        df_val.to_csv(\"temp_val.tsv\", sep='\\t', header=False, index=False)\n",
        "\n",
        "        # Train the model on the current fold\n",
        "        model = training_pipeline(alg, \"temp_train.tsv\")\n",
        "        # Predict on the validation fold\n",
        "        y_pred = predict(model, \"temp_val.tsv\")\n",
        "        # Recalculate the true labels for validation set (like read_data does)\n",
        "        y_true = df_val.apply(\n",
        "            lambda row: 0 if row['user_handle'] == 'realDonaldTrump' and str(row['device']).lower() == 'android' else 1,\n",
        "            axis=1\n",
        "        ).tolist()\n",
        "\n",
        "        # Evaluate metrics\n",
        "        acc_scores.append(accuracy_score(y_true, y_pred))\n",
        "        f1_scores.append(f1_score(y_true, y_pred))\n",
        "        precision_scores.append(precision_score(y_true, y_pred))\n",
        "        recall_scores.append(recall_score(y_true, y_pred))\n",
        "\n",
        "    return (\n",
        "        np.mean(acc_scores),\n",
        "        np.mean(precision_scores),\n",
        "        np.mean(recall_scores),\n",
        "        np.mean(f1_scores),\n",
        "\n",
        "    )\n"
      ],
      "metadata": {
        "id": "QmItAAofv0nL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions_to_file(predictions, filename):\n",
        "    \"\"\"\n",
        "    Saves a list of predictions to a file as a space-separated string.\n",
        "\n",
        "    Parameters:\n",
        "    - predictions: list or array-like of int - predicted labels to save\n",
        "    - filename: str - path to the output file\n",
        "\n",
        "    Writes:\n",
        "    - A single line with space-separated prediction values.\n",
        "    \"\"\"\n",
        "    # Convert list of ints to space-separated string\n",
        "    prediction_line = ' '.join(str(p) for p in predictions)\n",
        "    # Write to file\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(prediction_line)"
      ],
      "metadata": {
        "id": "dOdP7_cX_TZ4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Runs the full training, evaluation, and prediction pipeline for five different models.\n",
        "\n",
        "    Workflow:\n",
        "    - Performs 5-fold cross-validation on each algorithm (Logistic Regression, SVM, FFNN, XGBoost, RoBERTa)\n",
        "    - Prints accuracy, precision, recall, and F1-score for each\n",
        "    - Trains each model on full training data and predicts on test data\n",
        "    - Saves test predictions to separate output files\n",
        "\n",
        "    Files:\n",
        "    - Input: trump_train.tsv, trump_tweets_test_a.tsv\n",
        "    - Output: results_alg1.txt to results_alg5.txt\n",
        "    \"\"\"\n",
        "    train_path = \"/content/trump_train.tsv\"\n",
        "    test_path = \"/content/trump_tweets_test_a.tsv\"\n",
        "    print(\"========== 5-Fold Cross-Validation Results ==========\\n\")\n",
        "\n",
        "    model_names = {\n",
        "        1: \"Logistic Regression\",\n",
        "        2: \"SVM\",\n",
        "        3: \"FFNN\",\n",
        "        4:\"XGBoost\",\n",
        "        5:\"Roberta\"\n",
        "    }\n",
        "\n",
        "    for alg_id, name in model_names.items():\n",
        "        print(f\"▶ Evaluating: {name}\")\n",
        "        acc,precision,recall, f1 = cross_validate_model(alg_id, train_path, k=5)\n",
        "        print(f\"  Accuracy: {acc:.2f}\")\n",
        "        print(f\"  Precision: {precision:.2f}\")\n",
        "        print(f\"  Recall:    {recall:.2f}\\n\")\n",
        "        print(f\"  F1 Score: {f1:.2f}\\n\")\n",
        "\n",
        "    for alg in range(1,6):\n",
        "      print(f'algo:{alg}')\n",
        "      best_model_test = training_pipeline(alg,train_path)\n",
        "      test_pred = predict(best_model_test, test_path)\n",
        "      print(f'test predictions: {test_pred}')\n",
        "      print(f'len predictions: {len(test_pred)}')\n",
        "      output_filename = f'results_alg{alg}.txt'\n",
        "      save_predictions_to_file(test_pred, output_filename)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "iGivgam9wUuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92e3101c-3027-4082-9b2c-17213ae44d10"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== 5-Fold Cross-Validation Results ==========\n",
            "\n",
            "▶ Evaluating: Logistic Regression\n",
            "  Accuracy: 0.85\n",
            "  Precision: 0.87\n",
            "  Recall:    0.65\n",
            "\n",
            "  F1 Score: 0.74\n",
            "\n",
            "▶ Evaluating: SVM\n",
            "  Accuracy: 0.84\n",
            "  Precision: 0.90\n",
            "  Recall:    0.61\n",
            "\n",
            "  F1 Score: 0.73\n",
            "\n",
            "▶ Evaluating: FFNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Accuracy: 0.86\n",
            "  Precision: 0.85\n",
            "  Recall:    0.72\n",
            "\n",
            "  F1 Score: 0.78\n",
            "\n",
            "▶ Evaluating: XGBoost\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [14:34:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [14:34:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [14:34:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [14:34:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [14:34:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Accuracy: 0.88\n",
            "  Precision: 0.86\n",
            "  Recall:    0.76\n",
            "\n",
            "  F1 Score: 0.81\n",
            "\n",
            "▶ Evaluating: Roberta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0\n",
            "epoch:1\n",
            "epoch:2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0\n",
            "epoch:1\n",
            "epoch:2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0\n",
            "epoch:1\n",
            "epoch:2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0\n",
            "epoch:1\n",
            "epoch:2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0\n",
            "epoch:1\n",
            "epoch:2\n",
            "  Accuracy: 0.91\n",
            "  Precision: 0.95\n",
            "  Recall:    0.78\n",
            "\n",
            "  F1 Score: 0.86\n",
            "\n",
            "algo:1\n",
            "test predictions: [1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 0 1\n",
            " 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            "len predictions: 200\n",
            "algo:2\n",
            "test predictions: [1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0]\n",
            "len predictions: 200\n",
            "algo:3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test predictions: [0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "len predictions: 200\n",
            "algo:4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [15:39:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test predictions: [1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1\n",
            " 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0\n",
            " 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            "len predictions: 200\n",
            "algo:5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0\n",
            "epoch:1\n",
            "epoch:2\n",
            "test predictions: [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "len predictions: 200\n"
          ]
        }
      ]
    },
    {
